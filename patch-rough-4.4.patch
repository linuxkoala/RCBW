From c454468125846554c1ee8292c4a4f37491c265ee Mon Sep 17 00:00:00 2001
From: Yupeng Li <ypli15@lzu.edu.cn>
Date: Sun, 23 Jul 2017 23:10:43 +0800
Subject: [PATCH] Update RCBW patch for linux-4.4 kernel

Signed-off-by: Yupeng Li <ypli15@lzu.edu.cn>
---
 include/linux/sched.h |   5 +
 init/Kconfig          |  15 ++
 kernel/sched/core.c   | 302 +++++++++++++++++++++-
 kernel/sched/fair.c   | 701 +++++++++++++++++++++++++++++++++++++++++++++++++-
 kernel/sched/sched.h  |  41 +++
 5 files changed, 1047 insertions(+), 17 deletions(-)

diff --git a/include/linux/sched.h b/include/linux/sched.h
index fa39434..9387dc7 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -1265,6 +1265,11 @@ struct sched_entity {
 	struct cfs_rq		*cfs_rq;
 	/* rq "owned" by this entity/group: */
 	struct cfs_rq		*my_q;
+#ifdef CONFIG_CFS_BANDWIDTH_RESERVE
+    struct rb_node reserve_node;
+    int reserve_on_rq;
+    int enqueue_mode;
+#endif
 #endif
 
 #ifdef CONFIG_SMP
diff --git a/init/Kconfig b/init/Kconfig
index 235c7a2..038140f 100644
--- a/init/Kconfig
+++ b/init/Kconfig
@@ -1103,6 +1103,21 @@ config CFS_BANDWIDTH
 	  restriction.
 	  See tip/Documentation/scheduler/sched-bwc.txt for more information.
 
+config CFS_BANDWIDTH_RESERVE
+    bool "RCBW---The reservation of CPU bandwidth"
+    depends on !SMP && CFS_BANDWIDTH
+    default n
+    help
+        This option guarantees the runtime of a specific group of processes
+        when the group contend CPU resources with others.
+
+config RESERVE_DEBUG_INFO
+    bool "See some debug information about RCBW through Sysfs"
+    depends on CFS_BANDWIDTH_RESERVE
+    default n
+    help
+        See some debug information about RCBW throught Sysfs
+
 config RT_GROUP_SCHED
 	bool "Group scheduling for SCHED_RR/FIFO"
 	depends on CGROUP_SCHED
diff --git a/kernel/sched/core.c b/kernel/sched/core.c
index 732e993..2632164 100644
--- a/kernel/sched/core.c
+++ b/kernel/sched/core.c
@@ -7419,6 +7419,10 @@ void __init sched_init(void)
 
 #endif /* CONFIG_CGROUP_SCHED */
 
+#ifdef CONFIG_CFS_BANDWIDTH_RESERVE
+    init_cfs_bandwidth_reserve(&root_task_group.cfs_bandwidth);
+#endif
+
 	for_each_possible_cpu(i) {
 		struct rq *rq;
 
@@ -8290,6 +8294,7 @@ const u64 max_cfs_quota_period = 1 * NSEC_PER_SEC; /* 1s */
 const u64 min_cfs_quota_period = 1 * NSEC_PER_MSEC; /* 1ms */
 
 static int __cfs_schedulable(struct task_group *tg, u64 period, u64 runtime);
+static int create_reserve_info(struct task_group *tg);
 
 static int tg_set_cfs_bandwidth(struct task_group *tg, u64 period, u64 quota)
 {
@@ -8331,8 +8336,13 @@ static int tg_set_cfs_bandwidth(struct task_group *tg, u64 period, u64 quota)
 	 * If we need to toggle cfs_bandwidth_used, off->on must occur
 	 * before making related changes, and on->off must occur afterwards
 	 */
-	if (runtime_enabled && !runtime_was_enabled)
-		cfs_bandwidth_usage_inc();
+	if (runtime_enabled && !runtime_was_enabled) {
+        cfs_bandwidth_usage_inc();
+#ifdef CONFIG_RESERVE_DEBUG_INFO
+        create_reserve_info(tg);
+#endif
+    }
+
 	raw_spin_lock_irq(&cfs_b->lock);
 	cfs_b->period = ns_to_ktime(period);
 	cfs_b->quota = quota;
@@ -8390,12 +8400,22 @@ long tg_get_cfs_quota(struct task_group *tg)
 	return quota_us;
 }
 
+static int tg_set_cfs_bandwidth_reserve(struct task_group *tg,
+            u64 period, u64 reserve);
+
 int tg_set_cfs_period(struct task_group *tg, long cfs_period_us)
 {
-	u64 quota, period;
+	u64 quota, period, reserve;
 
 	period = (u64)cfs_period_us * NSEC_PER_USEC;
 	quota = tg->cfs_bandwidth.quota;
+#ifdef CONFIG_CFS_BANDWIDTH_RESERVE
+    if (tg->cfs_bandwidth.reserve_enabled == 1) {
+        reserve = tg->cfs_bandwidth.reserve;
+
+        return tg_set_cfs_bandwidth_reserve(tg, period, reserve);
+    }
+#endif 
 
 	return tg_set_cfs_bandwidth(tg, period, quota);
 }
@@ -8523,6 +8543,275 @@ static int cpu_stats_show(struct seq_file *sf, void *v)
 
 	return 0;
 }
+
+#ifdef CONFIG_CFS_BANDWIDTH_RESERVE
+
+static DEFINE_MUTEX(cfs_reserve_constraints_mutex);
+const u64 min_cfs_reserve_period = 1 * NSEC_PER_MSEC; /* 1ms */
+
+long tg_get_cfs_reserve(struct task_group *tg)
+{
+	u64 reserve_us;
+
+	if (tg->cfs_bandwidth.reserve == RESERVE_INF)
+		return -1;
+
+	reserve_us = tg->cfs_bandwidth.reserve;
+	do_div(reserve_us, NSEC_PER_USEC);
+
+	return reserve_us;
+}
+
+static s64 cpu_cfs_reserve_read_s64(struct cgroup_subsys_state *css,
+                        struct cftype *cft)
+{
+    return tg_get_cfs_reserve(css_tg(css));
+}
+
+static int tg_reserve_ndown(struct task_group *tg, void *d)
+{
+    return 0;
+}
+
+static int tg_reserve_up(struct task_group *tg, void *d)
+{
+    struct cfs_reserve_schedulable_data *data = d;
+    struct task_group *p_tg = tg->parent;
+    struct cfs_bandwidth *pcfs_b = &p_tg->cfs_bandwidth;
+    
+    u64 p_reserve, c_reserve;
+    u64 total;
+
+    if (tg == &root_task_group)
+        return 0;
+
+    p_reserve = normalize_cfs_reserve(p_tg, NULL);
+    c_reserve = normalize_cfs_reserve(tg, NULL);
+    if (p_reserve < c_reserve || p_reserve < pcfs_b->total_children_reserve)
+        return -EINVAL;
+
+    if (tg == data->tg) {
+        u64 curr_reserve = normalize_cfs_reserve(NULL, d);
+        
+        total = pcfs_b->total_children_reserve;
+        total += curr_reserve - c_reserve;
+        if(total > p_reserve)
+            return -EINVAL;
+
+        raw_spin_lock(&pcfs_b->lock); 
+        pcfs_b->total_children_reserve = total;
+        raw_spin_unlock(&pcfs_b->lock);
+    }
+    return 0;
+}
+
+static int __cfs_reserve_schedulable(struct task_group *tg, 
+                    u64 reserve, u64 period)
+{
+    int ret = 0;
+    struct task_group *p_tg = tg->parent;
+    struct cfs_reserve_schedulable_data data = {
+        .tg = tg,
+        .period = period,
+        .reserve = reserve,
+    };
+
+    if (p_tg != &root_task_group && !p_tg->cfs_bandwidth.reserve_enabled) 
+        return -EPERM;
+    
+    rcu_read_lock();
+    ret = walk_tg_tree(tg_reserve_ndown, tg_reserve_up, &data);
+    rcu_read_unlock();
+    
+    return ret;
+       
+}
+
+/* units are ns */
+static int check_quota_reserve(u64 quota, u64 reserve) 
+{
+    if (reserve == RESERVE_INF)
+        return 0;
+
+    return quota < reserve ? 1 : 0;
+}
+
+#ifdef CONFIG_RESERVE_DEBUG_INFO
+#include <linux/kobject.h>
+#include <linux/sysfs.h>
+
+#include <linux/cgroup.h>
+#include <linux/cgroup-defs.h>
+
+struct tg_kobj {
+    struct kobject kobj;
+    struct task_group *tg;
+};
+
+#define TO_TG_KOBJ(pkobj) container_of(pkobj, struct tg_kobj, kobj)
+
+static ssize_t se_info_show(struct kobject *kobj, struct kobj_attribute *attr,
+                           char *buf)
+{
+    struct tg_kobj *tmp = TO_TG_KOBJ(kobj);
+    struct task_group *tg = tmp->tg;
+    int i, count = 0;
+
+    for_each_possible_cpu(i) {
+        struct sched_entity *se = tg->se[i];
+        struct cfs_rq *cfs_rq = se->my_q;
+        struct cfs_rq *pcfs_rq = se->cfs_rq;
+        struct rq *rq = cfs_rq->rq;
+        
+        count += sprintf(buf+count, "enqueue mode :%d\n", se->enqueue_mode);
+        count += sprintf(buf+count, "on reserve runqueue:%d\n",se->reserve_on_rq); 
+    
+    }
+
+    return count;
+}
+
+static ssize_t cfs_rq_info_show(struct kobject *kobj, struct kobj_attribute *attr,
+                            char *buf)
+{
+    struct tg_kobj *tmp = TO_TG_KOBJ(kobj);
+    struct task_group *tg= tmp->tg;
+    struct cfs_bandwidth *cfs_b = &tg->cfs_bandwidth;
+    int i, count = 0;
+
+    for_each_possible_cpu(i) {
+        struct cfs_rq *cfs_rq = tg->cfs_rq[i];
+        
+        count += sprintf(buf+count,"count:%d\n", cfs_rq->throttled_count);
+        count += sprintf(buf+count, "remaining:%d\n", cfs_rq->reserve_remaining);
+        count += sprintf(buf+count, "reserve_b:%d\n", cfs_b->reserve);
+        count += sprintf(buf+count, "period:%d\n",cfs_b->reserve_period);
+    }
+
+    return count;
+}
+
+static struct kobject *reserve_info;
+
+static struct kobj_attribute se_attribute = 
+        __ATTR_RO(se_info);
+
+static struct kobj_attribute cfs_rq_attribute = 
+        __ATTR_RO(cfs_rq_info);
+
+static struct attribute *attrs[] = {
+    &se_attribute.attr,
+    &cfs_rq_attribute.attr,
+    NULL,
+};
+
+static struct attribute_group attr_group = {
+    .attrs = attrs,
+};
+
+
+static int create_reserve_info(struct task_group *tg)
+{   
+    struct cgroup *cgroup;
+    struct tg_kobj *tg_kobj;
+    char buf[20];
+    int ret;
+
+    tg_kobj = kzalloc(sizeof(*tg_kobj), GFP_KERNEL);
+    tg_kobj->tg = tg;
+
+    cgroup = tg->css.cgroup;
+    cgroup_name(cgroup, buf, 20);
+    
+    ret = kobject_init_and_add(&tg_kobj->kobj, kernel_kobj->ktype, 
+                               kernel_kobj, buf);
+    if (ret)
+        goto err_handle;
+    kobject_uevent(&tg_kobj->kobj, KOBJ_ADD);
+    
+    ret = sysfs_create_group(&tg_kobj->kobj, &attr_group);
+    if (ret)
+        goto err_handle;
+    
+    return 0;
+
+err_handle:
+    kobject_put(&tg_kobj->kobj);
+    kfree(tg_kobj);
+    return ret;
+    
+}
+
+#endif
+
+/* units are ns */
+static int tg_set_cfs_bandwidth_reserve(struct task_group *tg,
+                        u64 period, u64 reserve)
+{
+    struct cfs_bandwidth *cfs_b = &tg->cfs_bandwidth;   
+    int ret = 0, reserve_was_enabled, reserve_enabled; 
+    long quota = tg_get_cfs_quota(tg);
+
+    if (tg == &root_task_group) 
+        return -EINVAL;
+    
+    if (quota < 0) 
+        return -EPERM;
+    
+    if (period < min_cfs_quota_period || period > max_cfs_quota_period) 
+        return -EINVAL;
+
+    if (reserve < min_cfs_reserve_period)
+        return -EINVAL;
+    
+    if (reserve > period && reserve != RESERVE_INF)
+        return -EINVAL;
+    
+    mutex_lock(&cfs_reserve_constraints_mutex);
+    
+    ret = __cfs_reserve_schedulable(tg, reserve, period);
+    if(ret)
+        goto reserve_unlock;
+    
+    quota = quota * NSEC_PER_USEC;
+    reserve_enabled = check_quota_reserve(quota, reserve);
+    reserve_was_enabled = check_quota_reserve(cfs_b->quota, cfs_b->reserve);
+
+    raw_spin_lock_irq(&cfs_b->lock);
+    cfs_b->reserve = reserve;
+    cfs_b->reserve_enabled = reserve_enabled;
+    raw_spin_unlock_irq(&cfs_b->lock);
+    
+    /* reserve and period parameters are rational */
+    if(reserve_enabled) 
+        transform_to_reserve(tg, reserve, period);
+    
+reserve_unlock:
+    mutex_unlock(&cfs_reserve_constraints_mutex);
+
+    return ret;
+}
+
+static int tg_set_cfs_reserve(struct task_group *tg, s64 cfs_reserve_us)
+{
+    u64 reserve, period;
+
+	period = ktime_to_ns(tg->cfs_bandwidth.period);
+	if (cfs_reserve_us < 0)
+		reserve = RESERVE_INF;
+	else
+		reserve = (u64)cfs_reserve_us * NSEC_PER_USEC;
+
+	return tg_set_cfs_bandwidth_reserve(tg, period, reserve); 
+}
+
+static int cpu_cfs_reserve_write_s64(struct cgroup_subsys_state *css,
+            struct cftype *cftype, s64 cfs_reserve_us)
+{
+	return tg_set_cfs_reserve(css_tg(css), cfs_reserve_us);   
+}
+
+#endif /* CONFIG_CFS_BANDWIDTH_RESERVE */
 #endif /* CONFIG_CFS_BANDWIDTH */
 #endif /* CONFIG_FAIR_GROUP_SCHED */
 
@@ -8575,6 +8864,13 @@ static struct cftype cpu_files[] = {
 		.name = "stat",
 		.seq_show = cpu_stats_show,
 	},
+#ifdef CONFIG_CFS_BANDWIDTH_RESERVE
+    {
+        .name = "cfs_reserve_us",
+        .read_s64 = cpu_cfs_reserve_read_s64,
+        .write_s64 = cpu_cfs_reserve_write_s64,
+    },
+#endif
 #endif
 #ifdef CONFIG_RT_GROUP_SCHED
 	{
diff --git a/kernel/sched/fair.c b/kernel/sched/fair.c
index cfdc0e6..1654c5c 100644
--- a/kernel/sched/fair.c
+++ b/kernel/sched/fair.c
@@ -426,6 +426,8 @@ find_matching_se(struct sched_entity **se, struct sched_entity **pse)
 static __always_inline
 void account_cfs_rq_runtime(struct cfs_rq *cfs_rq, u64 delta_exec);
 
+static void account_cfs_rq_reserve_time(struct cfs_rq *cfs_rq, u64 delta_exec);
+
 /**************************************************************
  * Scheduling class tree data structure manipulation methods:
  */
@@ -731,6 +733,7 @@ static void update_curr(struct cfs_rq *cfs_rq)
 	}
 
 	account_cfs_rq_runtime(cfs_rq, delta_exec);
+    account_cfs_rq_reserve_time(cfs_rq, delta_exec);
 }
 
 static void update_curr_fair(struct rq *rq)
@@ -2994,11 +2997,14 @@ place_entity(struct cfs_rq *cfs_rq, struct sched_entity *se, int initial)
 }
 
 static void check_enqueue_throttle(struct cfs_rq *cfs_rq);
+static bool check_cfs_rq_reservationtime(struct cfs_rq *cfs_rq);
+static void check_enqueue_reserve_entity(struct cfs_rq *cfs_rq, 
+                struct sched_entity *se, int flags);
 
 static void
 enqueue_entity(struct cfs_rq *cfs_rq, struct sched_entity *se, int flags)
 {
-	/*
+    /*
 	 * Update the normalized vruntime before updating min_vruntime
 	 * through calling update_curr().
 	 */
@@ -3020,11 +3026,18 @@ enqueue_entity(struct cfs_rq *cfs_rq, struct sched_entity *se, int flags)
 
 	update_stats_enqueue(cfs_rq, se);
 	check_spread(cfs_rq, se);
+
 	if (se != cfs_rq->curr)
-		__enqueue_entity(cfs_rq, se);
-	se->on_rq = 1;
+#ifdef CONFIG_CFS_BANDWIDTH_RESERVE
+        check_enqueue_reserve_entity(cfs_rq, se, flags);
+#else
+        __enqueue_entity(cfs_rq, se);
+#endif
+    se->on_rq = 1;
+
+    check_cfs_rq_reservationtime(cfs_rq);
 
-	if (cfs_rq->nr_running == 1) {
+    if (cfs_rq->nr_running == 1) {
 		list_add_leaf_cfs_rq(cfs_rq);
 		check_enqueue_throttle(cfs_rq);
 	}
@@ -3076,6 +3089,8 @@ static void clear_buddies(struct cfs_rq *cfs_rq, struct sched_entity *se)
 }
 
 static __always_inline void return_cfs_rq_runtime(struct cfs_rq *cfs_rq);
+static void check_dequeue_reserve_entity(struct cfs_rq *cfs_rq, 
+                                struct sched_entity *se);
 
 static void
 dequeue_entity(struct cfs_rq *cfs_rq, struct sched_entity *se, int flags)
@@ -3103,7 +3118,11 @@ dequeue_entity(struct cfs_rq *cfs_rq, struct sched_entity *se, int flags)
 	clear_buddies(cfs_rq, se);
 
 	if (se != cfs_rq->curr)
+#ifdef CONFIG_CFS_BANDWIDTH_RESERVE
+        check_dequeue_reserve_entity(cfs_rq, se);
+#else
 		__dequeue_entity(cfs_rq, se);
+#endif
 	se->on_rq = 0;
 	account_entity_dequeue(cfs_rq, se);
 
@@ -3131,6 +3150,13 @@ check_preempt_tick(struct cfs_rq *cfs_rq, struct sched_entity *curr)
 	unsigned long ideal_runtime, delta_exec;
 	struct sched_entity *se;
 	s64 delta;
+#ifdef CONFIG_CFS_BANDWIDTH_RESERVE
+    struct cfs_rq *my_q;
+    
+    my_q = group_cfs_rq(curr);
+    if (my_q && my_q->reserve_enabled && my_q->reserve_remaining > 0)
+        return;
+#endif
 
 	ideal_runtime = sched_slice(cfs_rq, curr);
 	delta_exec = curr->sum_exec_runtime - curr->prev_sum_exec_runtime;
@@ -3173,7 +3199,11 @@ set_next_entity(struct cfs_rq *cfs_rq, struct sched_entity *se)
 		 * runqueue.
 		 */
 		update_stats_wait_end(cfs_rq, se);
-		__dequeue_entity(cfs_rq, se);
+#ifdef CONFIG_CFS_BANDWIDTH_RESERVE
+        check_dequeue_reserve_entity(cfs_rq, se);
+#else
+        __dequeue_entity(cfs_rq, se);
+#endif
 		update_load_avg(se, 1);
 	}
 
@@ -3195,6 +3225,7 @@ set_next_entity(struct cfs_rq *cfs_rq, struct sched_entity *se)
 
 static int
 wakeup_preempt_entity(struct sched_entity *curr, struct sched_entity *se);
+static struct sched_entity *__pick_first_reserve_entity(struct cfs_rq *cfs_rq);
 
 /*
  * Pick the next process, keeping these things in mind, in this order:
@@ -3208,6 +3239,9 @@ pick_next_entity(struct cfs_rq *cfs_rq, struct sched_entity *curr)
 {
 	struct sched_entity *left = __pick_first_entity(cfs_rq);
 	struct sched_entity *se;
+#ifdef CONFIG_CFS_BANDWIDTH_RESERVE
+    struct sched_entity *reserve_left;
+#endif
 
 	/*
 	 * If curr is set we have to see if its left of the leftmost entity
@@ -3237,6 +3271,11 @@ pick_next_entity(struct cfs_rq *cfs_rq, struct sched_entity *curr)
 			se = second;
 	}
 
+#ifdef CONFIG_CFS_BANDWIDTH_RESERVE
+    reserve_left = __pick_first_reserve_entity(cfs_rq);
+    if (reserve_left) 
+        se = reserve_left;
+#endif
 	/*
 	 * Prefer last buddy, try to return the CPU to a preempted task.
 	 */
@@ -3267,12 +3306,18 @@ static void put_prev_entity(struct cfs_rq *cfs_rq, struct sched_entity *prev)
 
 	/* throttle cfs_rqs exceeding runtime */
 	check_cfs_rq_runtime(cfs_rq);
+    
+    check_cfs_rq_reservationtime(cfs_rq);
 
 	check_spread(cfs_rq, prev);
 	if (prev->on_rq) {
 		update_stats_wait_start(cfs_rq, prev);
 		/* Put 'current' back into the tree. */
-		__enqueue_entity(cfs_rq, prev);
+#ifdef CONFIG_CFS_BANDWIDTH_RESERVE
+        check_enqueue_reserve_entity(cfs_rq, prev, 0);
+#else
+        __enqueue_entity(cfs_rq, prev);
+#endif
 		/* in !on_rq case, update occurred at dequeue */
 		update_load_avg(prev, 0);
 	}
@@ -3309,8 +3354,8 @@ entity_tick(struct cfs_rq *cfs_rq, struct sched_entity *curr, int queued)
 			hrtimer_active(&rq_of(cfs_rq)->hrtick_timer))
 		return;
 #endif
-
-	if (cfs_rq->nr_running > 1)
+	
+    if (cfs_rq->nr_running > 1)
 		check_preempt_tick(cfs_rq, curr);
 }
 
@@ -3926,6 +3971,11 @@ static enum hrtimer_restart sched_cfs_slack_timer(struct hrtimer *timer)
 	struct cfs_bandwidth *cfs_b =
 		container_of(timer, struct cfs_bandwidth, slack_timer);
 
+#ifdef CONFIG_CFS_BANDWIDTH_RESERVE    
+    if (cfs_b->reserve_enabled) 
+        return HRTIMER_NORESTART;
+#endif
+
 	do_sched_cfs_slack_timer(cfs_b);
 
 	return HRTIMER_NORESTART;
@@ -3939,6 +3989,12 @@ static enum hrtimer_restart sched_cfs_period_timer(struct hrtimer *timer)
 	int idle = 0;
 
 	raw_spin_lock(&cfs_b->lock);
+#ifdef CONFIG_CFS_BANDWIDTH_RESERVE    
+    if (cfs_b->reserve_enabled) {
+        raw_spin_unlock(&cfs_b->lock);
+        return HRTIMER_NORESTART;
+    }
+#endif
 	for (;;) {
 		overrun = hrtimer_forward_now(timer, cfs_b->period);
 		if (!overrun)
@@ -3965,12 +4021,16 @@ void init_cfs_bandwidth(struct cfs_bandwidth *cfs_b)
 	cfs_b->period_timer.function = sched_cfs_period_timer;
 	hrtimer_init(&cfs_b->slack_timer, CLOCK_MONOTONIC, HRTIMER_MODE_REL);
 	cfs_b->slack_timer.function = sched_cfs_slack_timer;
+
+    init_cfs_bandwidth_reserve(cfs_b);
 }
 
 static void init_cfs_rq_runtime(struct cfs_rq *cfs_rq)
 {
 	cfs_rq->runtime_enabled = 0;
 	INIT_LIST_HEAD(&cfs_rq->throttled_list);
+    
+    init_cfs_rq_reserve(cfs_rq);
 }
 
 void start_cfs_bandwidth(struct cfs_bandwidth *cfs_b)
@@ -4031,6 +4091,595 @@ static void __maybe_unused unthrottle_offline_cfs_rqs(struct rq *rq)
 	}
 }
 
+/**************************************************
+ * the reservation of CPU bandwidth control machinery
+ */ 
+#ifdef CONFIG_CFS_BANDWIDTH_RESERVE
+
+static void __refill_cfs_bandwidth_reservationtime(struct cfs_rq *cfs_rq,
+                    struct task_group *tg)
+{
+    struct cfs_bandwidth *cfs_b = &tg->cfs_bandwidth;
+    struct rq *rq = rq_of(cfs_rq);
+    u64 now;
+
+    now = rq_clock(rq);
+    cfs_rq->reserve_remaining += cfs_b->reserve;
+    cfs_rq->reserve_expires = now + cfs_b->reserve_period;
+}
+
+struct sched_entity *__pick_first_reserve_entity(struct cfs_rq *cfs_rq)
+{
+	struct rb_node *left = cfs_rq->reserve_leftmost;
+
+	if (!left)
+		return NULL;
+    
+	return rb_entry(left, struct sched_entity, reserve_node);
+}
+
+void init_cfs_bandwidth_reserve(struct cfs_bandwidth *cfs_b)
+{
+    struct task_group *tg = container_of(cfs_b, struct task_group, cfs_bandwidth);
+
+    cfs_b->reserve = RESERVE_INF;
+    cfs_b->reserve_period = ktime_to_ns(cfs_b->period);
+      
+    cfs_b->reserve_enabled = 0;
+    cfs_b->total_children_reserve = 0;
+
+    INIT_LIST_HEAD(&cfs_b->reserve_throttled_list);
+}
+
+static enum hrtimer_restart sched_cfs_reserve_period_timer(struct hrtimer *timer);
+
+void init_cfs_rq_reserve(struct cfs_rq *cfs_rq)
+{
+    cfs_rq->reserve_root = RB_ROOT;
+    cfs_rq->reserve_remaining = 0;
+    cfs_rq->reserve_expires = RESERVE_EXPIRES_INF;
+    cfs_rq->reserve_enabled = 0;
+    cfs_rq->reserve_throttled = 0;
+    cfs_rq->reserve_active = 0;
+    INIT_LIST_HEAD(&cfs_rq->reserve_throttled_node);
+    
+    hrtimer_init(&cfs_rq->reserve_period_timer, 
+                CLOCK_MONOTONIC, HRTIMER_MODE_ABS_PINNED);
+	cfs_rq->reserve_period_timer.function = sched_cfs_reserve_period_timer;
+}
+
+u64 normalize_cfs_reserve(struct task_group *tg, 
+                struct cfs_reserve_schedulable_data *data)
+{
+    u64 reserve, period;
+    u64 default_period = default_cfs_period();
+
+    if (tg == &root_task_group)
+        return default_period;
+
+    if (tg == NULL) {
+        reserve = data->reserve;
+        period = data->period;
+    } else { 
+        reserve = tg->cfs_bandwidth.reserve;
+        period = tg->cfs_bandwidth.reserve_period;
+    }
+
+    if (reserve == RESERVE_INF)
+        return 0;
+            
+    if (likely(period == default_period))
+        return reserve;
+    
+    reserve *= default_period;
+    do_div(reserve, period);
+
+    return reserve;
+
+}
+
+static void destroy_cfs_reserve(struct task_group *tg)
+{
+    struct cfs_bandwidth *cfs_b = &tg->cfs_bandwidth;
+    struct cfs_bandwidth *pcfs_b = &tg->parent->cfs_bandwidth;
+
+    if (cfs_b->reserve == RESERVE_INF)
+        return ;
+
+    raw_spin_lock(&pcfs_b->lock);
+    pcfs_b->total_children_reserve -= cfs_b->reserve;
+    raw_spin_unlock(&pcfs_b->lock);
+    
+    return ;
+}
+
+/* a and b must be process groups */
+bool reserve_expires_before(struct sched_entity *a, struct sched_entity *b)
+{
+    struct cfs_rq *acfs_rq = group_cfs_rq(a);
+    struct cfs_rq *bcfs_rq = group_cfs_rq(b);
+    
+    return acfs_rq->reserve_expires < bcfs_rq->reserve_expires;
+}
+
+bool reserve_expires_overflow(struct cfs_rq *cfs_rq, u64 now)
+{
+    struct task_group *tg = cfs_rq->tg;
+    struct cfs_bandwidth *cfs_b = &tg->cfs_bandwidth;
+
+    s64 reserve_remaining = cfs_rq->reserve_remaining;
+    u64 reserve_expires = cfs_rq->reserve_expires;
+    u64 left, right;
+
+    if (reserve_remaining <= 0)
+        return true;
+
+    left = (reserve_remaining >> RESERVE_PRECISION) * 
+                (cfs_b->reserve_period >> RESERVE_PRECISION);
+    right = ((reserve_expires - now) >> RESERVE_PRECISION) *
+                (cfs_b->reserve >> RESERVE_PRECISION);
+
+    return left > right;
+}
+
+static void __enqueue_reserve_entity(struct cfs_rq *cfs_rq, 
+                        struct sched_entity *res_se)
+{
+    struct rb_node **link = &cfs_rq->reserve_root.rb_node;
+    struct rb_node *parent = NULL;
+    struct sched_entity *entry;
+    int leftmost = 1;
+
+    while (*link) {
+        parent = *link;
+        entry = rb_entry(parent, struct sched_entity, reserve_node);
+                    
+        if (reserve_expires_before(res_se, entry)) {
+            link = &parent->rb_left;
+        } else {
+            link = &parent->rb_right;
+            leftmost = 0;
+        }  
+    }
+
+    if (leftmost)
+        cfs_rq->reserve_leftmost = &res_se->reserve_node;
+    
+    rb_link_node(&res_se->reserve_node, parent, link);
+    rb_insert_color(&res_se->reserve_node, &cfs_rq->reserve_root);
+}
+
+/*
+ * when a sched_entity(in fact which is a group process) wake up,
+ * the scheduler checks if :
+ *      reserve_remaining / (reserve_expires - now) > reserve / reserve_period
+ * or
+ *      reserve_expires > now,
+ * anyone is verified, the remaining reserve and expires of the sched_entity are 
+ * re-initalized as :
+ *     expires = now + reserve_period;
+ *     reserve_remaining =  reserve;
+ */ 
+static void update_cfs_rq_reserve(struct cfs_rq *cfs_rq)
+{
+    struct task_group *tg = cfs_rq->tg;
+    struct cfs_bandwidth *cfs_b = &tg->cfs_bandwidth;
+    struct rq *rq = rq_of(cfs_rq);
+    
+    u64 now = rq_clock(rq);
+    if (cfs_rq->reserve_expires < now || reserve_expires_overflow(cfs_rq, now)) {
+        cfs_rq->reserve_expires = now + cfs_b->reserve_period;
+        cfs_rq->reserve_remaining = cfs_b->reserve;
+    }   
+}
+
+static void replenish_cfs_rq_reserve(struct cfs_rq *cfs_rq)
+{
+    struct task_group *tg = cfs_rq->tg;
+    struct cfs_bandwidth *cfs_b = &tg->cfs_bandwidth;
+    struct rq *rq = rq_of(cfs_rq);
+
+    u64 now = rq_clock(rq);
+    
+    /* initalized reserve subsystem by cgroup filesystem */
+    if (cfs_rq->reserve_expires == RESERVE_EXPIRES_INF) {
+        cfs_rq->reserve_remaining = cfs_b->reserve;
+        cfs_rq->reserve_expires = now + cfs_b->reserve_period;
+        return ;
+    }
+
+    while (cfs_rq->reserve_remaining <= 0) {
+        cfs_rq->reserve_remaining += cfs_b->reserve;
+        cfs_rq->reserve_expires += cfs_b->reserve_period; 
+    }
+
+    if (now - cfs_rq->reserve_expires < 0) {
+        cfs_rq->reserve_remaining = cfs_b->reserve;
+        cfs_rq->reserve_expires = now + cfs_b->reserve_period;
+    }
+}
+
+static void enqueue_reserve_entity(struct cfs_rq *cfs_rq, 
+                struct sched_entity *se, int flags)
+{
+    /*
+     * update_cfs_rq_reserve and replenish_cfs_rq_reserve implement 
+     * Constant Bandwidth Server algorithm
+     */ 
+    struct cfs_rq *my_q = group_cfs_rq(se);
+
+    if (flags & ENQUEUE_WAKEUP)
+        update_cfs_rq_reserve(my_q);
+    else if (flags & ENQUEUE_REPLENISH)
+        replenish_cfs_rq_reserve(my_q);
+    
+    se->reserve_on_rq = 1;
+    __enqueue_reserve_entity(cfs_rq, se);
+}
+
+static void check_enqueue_reserve_entity(struct cfs_rq *cfs_rq, 
+                struct sched_entity *se, int flags)
+{
+    struct cfs_rq *my_q;
+
+    if (entity_is_task(se))
+        return __enqueue_entity(cfs_rq, se);
+
+    my_q = group_cfs_rq(se);
+    if (likely(!my_q->reserve_enabled)) 
+        return __enqueue_entity(cfs_rq, se);
+ 
+    return enqueue_reserve_entity(cfs_rq, se, flags);
+}
+
+static void __dequeue_reserve_entity(struct cfs_rq *cfs_rq, 
+                struct sched_entity *se)
+{
+    if (cfs_rq->reserve_leftmost == &se->reserve_node) {
+        struct rb_node *next_node;
+
+        next_node = rb_next(&se->reserve_node);
+        cfs_rq->reserve_leftmost = next_node;
+    }
+
+    rb_erase(&se->reserve_node, &cfs_rq->reserve_root);
+    RB_CLEAR_NODE(&se->reserve_node);
+}
+
+static void check_dequeue_reserve_entity(struct cfs_rq *cfs_rq,
+                struct sched_entity *se)
+{
+    struct cfs_rq *my_q;
+
+    if (entity_is_task(se)) 
+        return __dequeue_entity(cfs_rq, se);
+    
+    my_q = group_cfs_rq(se);
+    if (likely(!my_q->reserve_enabled)) 
+        return __dequeue_entity(cfs_rq, se);
+
+    se->reserve_on_rq = 0;
+    return __dequeue_reserve_entity(cfs_rq, se);
+}
+
+static inline int cfs_rq_reserve_throttled(struct cfs_rq *cfs_rq)
+{
+    return cfs_rq->reserve_throttled;
+}
+
+static void unthrottled_reserve_cfs_rq(struct cfs_rq *cfs_rq)
+{
+    struct rq *rq = rq_of(cfs_rq);
+    struct cfs_bandwidth *cfs_b = tg_cfs_bandwidth(cfs_rq->tg);
+    struct sched_entity *se;
+    int task_delta, enqueue = 1;
+    int flags = ENQUEUE_REPLENISH;
+
+    se = cfs_rq->tg->se[cpu_of(rq)];
+
+    cfs_rq->reserve_throttled = 0;
+    
+    update_rq_clock(rq);
+    
+    if (!cfs_rq->load.weight)
+        return ;
+
+    task_delta = cfs_rq->h_nr_running;
+    for_each_sched_entity(se) {
+        struct cfs_rq *qcfs_rq = cfs_rq_of(se);
+
+        if (se->on_rq)
+            enqueue = 0;
+
+        if (enqueue)
+            enqueue_entity(qcfs_rq, se, flags);
+        qcfs_rq->h_nr_running += task_delta;
+
+        if (cfs_rq_reserve_throttled(qcfs_rq))
+            break;
+        
+        flags = ENQUEUE_WAKING;
+    }
+
+    if (!se){
+        add_nr_running(rq, task_delta);
+        resched_curr(rq);
+        return ;
+    }
+
+    if (rq->curr == rq->idle && rq->cfs.nr_running)
+		resched_curr(rq);
+}
+
+static enum hrtimer_restart sched_cfs_reserve_period_timer(struct hrtimer *timer)
+{
+    struct cfs_rq *cfs_rq = container_of(timer, struct cfs_rq, 
+                                        reserve_period_timer);
+    struct rq *rq = rq_of(cfs_rq);
+    struct cfs_bandwidth *cfs_b = tg_cfs_bandwidth(cfs_rq->tg);
+    int reserve_throttled;
+
+    if (!cfs_rq->reserve_enabled)
+        return HRTIMER_NORESTART;
+    
+    raw_spin_lock(&rq->lock);
+    unthrottled_reserve_cfs_rq(cfs_rq);
+    raw_spin_unlock(&rq->lock);
+    
+    return HRTIMER_NORESTART; 
+}
+
+
+int start_reserve_period_timer(struct cfs_rq *cfs_rq)
+{
+    struct rq *rq = rq_of(cfs_rq);
+    struct hrtimer *timer = &cfs_rq->reserve_period_timer;
+    ktime_t expire, now;
+    s64 delta;
+
+    lockdep_assert_held(&rq->lock);
+
+    expire = ns_to_ktime(cfs_rq->reserve_expires);
+    now = hrtimer_cb_get_time(timer);
+    delta = ktime_to_ns(now) - rq_clock(rq);
+    expire = ktime_add_ns(expire, delta);
+
+    /*
+     *  A possible situation ： when throttled cfs_rq ran out
+     *  of reserve time, but new period start.
+     *
+     *  the expire of last period is smaller than now
+     */
+    if (ktime_us_delta(expire, now) < 0)
+        return 0;
+    
+    cfs_rq->throttled_count++;
+    hrtimer_start(&cfs_rq->reserve_period_timer, expire,
+                    HRTIMER_MODE_ABS_PINNED);
+
+    return 1;
+}
+
+static void throttle_reserve_cfs_rq(struct cfs_rq *cfs_rq)
+{
+    struct rq *rq = rq_of(cfs_rq);
+    struct cfs_bandwidth *cfs_b = tg_cfs_bandwidth(cfs_rq->tg);
+    struct sched_entity *se;
+    struct cfs_rq *pcfs_rq;
+    int task_delta, ret, dequeue = 1;
+
+    se = cfs_rq->tg->se[cpu_of(rq)];
+
+    task_delta = cfs_rq->h_nr_running;
+
+    for_each_sched_entity(se) {
+        struct cfs_rq *qcfs_rq = cfs_rq_of(se);
+
+        if (!se->on_rq)
+            break;
+
+        if (dequeue) 
+            dequeue_entity(qcfs_rq, se, DEQUEUE_SLEEP);
+        qcfs_rq->h_nr_running -= task_delta;
+
+        if (qcfs_rq->load.weight)
+            dequeue = 0;
+    }
+
+    if (!se)
+        sub_nr_running(rq, task_delta);
+    
+    cfs_rq->reserve_throttled = 1;
+
+    if (!start_reserve_period_timer(cfs_rq))
+        unthrottled_reserve_cfs_rq(cfs_rq);
+    
+}
+
+static bool check_cfs_rq_reservationtime(struct cfs_rq *cfs_rq)
+{
+    if (likely(!cfs_rq->reserve_enabled || cfs_rq->reserve_remaining > 0))
+        return false;
+
+    if (cfs_rq_reserve_throttled(cfs_rq))
+        return true;
+
+    throttle_reserve_cfs_rq(cfs_rq);
+    return true;
+}
+
+static void account_cfs_rq_reserve_time(struct cfs_rq *cfs_rq, u64 delta_exec)
+{
+    if(likely(!cfs_rq->reserve_enabled))
+        return ;
+    
+    if(unlikely(!cfs_rq->load.weight))
+        return ;
+    
+    cfs_rq->reserve_remaining -= delta_exec;
+    if(cfs_rq->reserve_remaining > 0)
+        return ;
+   
+    resched_curr(rq_of(cfs_rq));
+}
+
+static u64 recalcuate_quota(u64 remaining, u64 prevquota, u64 prevtotal)
+{
+    u64 newquota = remaining;
+   
+    newquota = newquota * prevquota;
+    do_div(newquota, prevtotal);
+
+    return newquota;
+}
+
+/* units are ns */
+static void balance_cfs_banwidth(struct task_group *tg)
+{
+    struct task_group *p_tg = tg->parent;
+    struct task_group *pos;
+    struct cfs_bandwidth *pcfs_b = &p_tg->cfs_bandwidth;
+    struct cfs_bandwidth *cfs_b = &tg->cfs_bandwidth;
+    u64 reserve_remaining, total_quota = 0;
+    u64 total_children_reserve = p_tg->cfs_bandwidth.total_children_reserve;
+
+    if (p_tg == &root_task_group) 
+        reserve_remaining = default_cfs_period();   
+    else  
+        reserve_remaining = pcfs_b->reserve;
+
+    rcu_read_lock();
+    list_for_each_entry_rcu(pos, &p_tg->children, siblings) {
+        u64 quota = pos->cfs_bandwidth.quota;
+        if (quota == RUNTIME_INF || tg == pos)
+            continue;        
+
+        total_quota += quota;
+    }
+    rcu_read_unlock();
+
+    if (total_quota == 0)
+        return ;
+
+    reserve_remaining -= total_children_reserve;
+
+    list_for_each_entry_rcu(pos, &p_tg->children, siblings) {
+        struct cfs_bandwidth *cfs_b = &pos->cfs_bandwidth;
+        u64 prevquota = cfs_b->quota;
+        u64 newquota;
+
+        if (prevquota == RUNTIME_INF || tg == pos)
+            continue ;
+        
+        newquota = recalcuate_quota(reserve_remaining, prevquota, total_quota);
+        if (newquota >= prevquota)
+            continue;
+
+        raw_spin_lock_irq(&cfs_b->lock);
+        cfs_b->quota = newquota;
+        raw_spin_unlock_irq(&cfs_b->lock);
+    }
+    rcu_read_unlock();
+}
+
+void transform_to_reserve(struct task_group *tg, u64 reserve, u64 period)
+{
+    struct cfs_bandwidth *cfs_b = &tg->cfs_bandwidth;
+    struct hrtimer *timer = &cfs_b->period_timer;
+    struct sched_entity *se = tg->se[0];
+    struct cfs_rq *cfs_rq = tg->cfs_rq[0];
+    struct cfs_rq *pcfs_rq = cfs_rq_of(se);
+    struct rq *rq = rq_of(cfs_rq);
+    int i, nr_periods, throttled;
+   
+    balance_cfs_banwidth(tg);
+
+    nr_periods = hrtimer_forward_now(timer, cfs_b->period);
+    throttled = !list_empty(&cfs_b->throttled_cfs_rq);
+    
+    raw_spin_lock_irq(&cfs_b->lock);
+    cfs_b->period = ns_to_ktime(period);
+    cfs_b->reserve_period = period;
+    
+    cfs_b->idle = 0;
+    cfs_b->period_active = 0;
+    cfs_b->nr_periods += nr_periods;
+    if (throttled) 
+        cfs_b->nr_throttled += nr_periods;
+    raw_spin_unlock_irq(&cfs_b->lock);
+    
+    raw_spin_lock_irq(&rq->lock); 
+    cfs_rq->runtime_remaining = 0;
+    cfs_rq->runtime_enabled = 0;        
+        
+    cfs_rq->reserve_enabled = 1;
+    cfs_rq->throttled_count = 0;
+    
+    /* at present the se is running */ 
+    if (pcfs_rq->curr == se) {
+        se->enqueue_mode = 1;
+        se->reserve_on_rq = 1;
+        __refill_cfs_bandwidth_reservationtime(cfs_rq, tg);    
+        raw_spin_unlock_irq(&rq->lock);
+        return ;
+    }
+
+    /* maybe on runqueue */
+    if (se->on_rq) {
+        se->enqueue_mode = 2;
+        __dequeue_entity(pcfs_rq, se);
+        enqueue_reserve_entity(pcfs_rq, se, ENQUEUE_REPLENISH);
+        raw_spin_unlock_irq(&rq->lock);
+        return ;
+    }
+        
+    /* 
+     * maybe throttled or sleeping , the later doesn't
+     * consider now.
+     */
+    if (cfs_rq->throttled) {
+        cfs_rq->throttled = 0;
+        raw_spin_lock(&cfs_b->lock);
+        list_del_rcu(&cfs_rq->throttled_list);
+        raw_spin_unlock(&cfs_b->lock);
+
+        walk_tg_tree_from(cfs_rq->tg, tg_nop, tg_unthrottle_up, (void *)rq);
+    }
+
+    se->enqueue_mode = 3;
+    unthrottled_reserve_cfs_rq(cfs_rq);
+    raw_spin_unlock_irq(&rq->lock);    
+}
+
+#else
+u64 normalize_cfs_reserve(struct task_group *tg, 
+                struct cfs_reserve_schedulable_data *data) {}
+void init_cfs_bandwidth_reserve(struct cfs_bandwidth *cfs_b) {}
+void init_cfs_rq_reserve(struct cfs_rq *cfs_rq) {}
+void transform_to_reserve(struct task_group *tg, u64 reserve, u64 period) {}
+
+static void check_enqueue_reserve_entity(struct cfs_rq *cfs_rq, 
+                        struct sched_entity *se, int flags) {}
+static void check_dequeue_reserve_entity(struct cfs_rq *cfs_rq,
+                        struct sched_entity *se) {}
+
+static struct sched_entity * __pick_first_reserve_entity(struct cfs_rq *cfs_rq) 
+{
+    return NULL;
+}
+static void account_cfs_rq_reserve_time(struct cfs_rq *cfs_rq, u64 delta_exec) {}
+
+static inline int cfs_rq_reserve_throttled(struct cfs_rq *cfs_rq)
+{
+    return 0;
+}
+
+static inline bool check_cfs_rq_reservationtime(struct cfs_rq *cfs_rq)
+{
+    return false;
+}
+#endif
+
 #else /* CONFIG_CFS_BANDWIDTH */
 static inline u64 cfs_rq_clock_task(struct cfs_rq *cfs_rq)
 {
@@ -4151,6 +4800,10 @@ enqueue_task_fair(struct rq *rq, struct task_struct *p, int flags)
 		*/
 		if (cfs_rq_throttled(cfs_rq))
 			break;
+       
+        if (cfs_rq_reserve_throttled(cfs_rq))
+            break;
+    
 		cfs_rq->h_nr_running++;
 
 		flags = ENQUEUE_WAKEUP;
@@ -4158,11 +4811,14 @@ enqueue_task_fair(struct rq *rq, struct task_struct *p, int flags)
 
 	for_each_sched_entity(se) {
 		cfs_rq = cfs_rq_of(se);
-		cfs_rq->h_nr_running++;
-
-		if (cfs_rq_throttled(cfs_rq))
+        cfs_rq->h_nr_running++;
+        
+        if (cfs_rq_throttled(cfs_rq))
 			break;
 
+        if(cfs_rq_reserve_throttled(cfs_rq))
+            break;
+		
 		update_load_avg(se, 1);
 		update_cfs_shares(cfs_rq);
 	}
@@ -4190,6 +4846,8 @@ static void dequeue_task_fair(struct rq *rq, struct task_struct *p, int flags)
 		cfs_rq = cfs_rq_of(se);
 		dequeue_entity(cfs_rq, se, flags);
 
+        if (cfs_rq_reserve_throttled(cfs_rq))
+            break;
 		/*
 		 * end evaluation on encountering a throttled cfs_rq
 		 *
@@ -4220,6 +4878,9 @@ static void dequeue_task_fair(struct rq *rq, struct task_struct *p, int flags)
 		cfs_rq = cfs_rq_of(se);
 		cfs_rq->h_nr_running--;
 
+        if (cfs_rq_reserve_throttled(cfs_rq))
+            break;
+
 		if (cfs_rq_throttled(cfs_rq))
 			break;
 
@@ -5243,9 +5904,18 @@ again:
 				goto simple;
 		}
 
-		se = pick_next_entity(cfs_rq, curr);
+        if (unlikely(check_cfs_rq_reservationtime(cfs_rq))) 
+            cfs_rq = cfs_rq_of(se);
+/*
+        if (unlikely(cfs_rq->reserve_enabled)) {    
+            cfs_rq = cfs_rq_of(se);
+            dequeue_entity(cfs_rq, se, DEQUEUE_SLEEP);
+        }       
+*/
+		se = pick_next_entity(cfs_rq, curr);   
 		cfs_rq = group_cfs_rq(se);
-	} while (cfs_rq);
+    
+    } while (cfs_rq);
 
 	p = task_of(se);
 
@@ -8094,7 +8764,9 @@ void free_fair_sched_group(struct task_group *tg)
 	int i;
 
 	destroy_cfs_bandwidth(tg_cfs_bandwidth(tg));
-
+#ifdef CONFIG_CFS_BANDWIDTH_RESERVE
+    destroy_cfs_reserve(tg);
+#endif
 	for_each_possible_cpu(i) {
 		if (tg->cfs_rq)
 			kfree(tg->cfs_rq[i]);
@@ -8105,6 +8777,7 @@ void free_fair_sched_group(struct task_group *tg)
 		}
 	}
 
+
 	kfree(tg->cfs_rq);
 	kfree(tg->se);
 }
diff --git a/kernel/sched/sched.h b/kernel/sched/sched.h
index b242775..c724390 100644
--- a/kernel/sched/sched.h
+++ b/kernel/sched/sched.h
@@ -84,6 +84,10 @@ static inline void update_cpu_load_active(struct rq *this_rq) { }
  */
 #define RUNTIME_INF	((u64)~0ULL)
 
+#define RESERVE_INF ((u64)~0ULL)
+#define RESERVE_EXPIRES_INF ((u64)~0ULL)
+#define RESERVE_PRECISION (10)
+
 static inline int idle_policy(int policy)
 {
 	return policy == SCHED_IDLE;
@@ -233,9 +237,27 @@ struct cfs_bandwidth {
 	/* statistics */
 	int nr_periods, nr_throttled;
 	u64 throttled_time;
+#ifdef CONFIG_CFS_BANDWIDTH_RESERVE
+    /* units are ns */
+    u64 reserve; /* maximum runtime in each period */
+    u64 reserve_period; /* how long is a period, equivalent with period */
+
+    int reserve_enabled;
+    u64 total_children_reserve;
+    
+    struct list_head reserve_throttled_list;
+#endif
 #endif
 };
 
+#ifdef CONFIG_CFS_BANDWIDTH_RESERVE
+struct cfs_reserve_schedulable_data {
+        struct task_group *tg;
+        u64 period, reserve;
+
+};
+#endif
+
 /* task group related information */
 struct task_group {
 	struct cgroup_subsys_state css;
@@ -333,6 +355,11 @@ extern void sched_offline_group(struct task_group *tg);
 
 extern void sched_move_task(struct task_struct *tsk);
 
+extern void init_cfs_bandwidth_reserve(struct cfs_bandwidth *cfs_b);
+extern void init_cfs_rq_reserve(struct cfs_rq *cfs_rq);
+extern u64 normalize_cfs_reserve(struct task_group *tg,
+                        struct cfs_reserve_schedulable_data *data);
+extern void transform_to_reserve(struct task_group *tg, u64 reserve, u64 period);
 #ifdef CONFIG_FAIR_GROUP_SCHED
 extern int sched_group_set_shares(struct task_group *tg, unsigned long shares);
 #endif
@@ -419,6 +446,20 @@ struct cfs_rq {
 	u64 throttled_clock_task_time;
 	int throttled, throttle_count;
 	struct list_head throttled_list;
+#ifdef CONFIG_CFS_BANDWIDTH_RESERVE
+    int reserve_active;
+    s64 reserve_remaining;
+    u64 reserve_expires;
+    int reserve_enabled;
+    int reserve_throttled;
+    int throttled_count;
+
+    struct rb_root reserve_root;
+    struct rb_node *reserve_leftmost;
+    struct list_head reserve_throttled_node;
+    
+    struct hrtimer reserve_period_timer;
+#endif /* CONFIG_CFS_BANDWIDTH_RESERVE */
 #endif /* CONFIG_CFS_BANDWIDTH */
 #endif /* CONFIG_FAIR_GROUP_SCHED */
 };
-- 
2.7.4

